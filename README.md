# Entropic Perspective in Neuroscience

One of the biggest challenges in understanding neuroscientific studies is their wide scope, which spans very different fields such as biology, mathematics, statistics, information theory, and linear algebra. This integration of multiple disciplines gives rise to the emergence of concepts that may initially appear irrelevant. However, these are in fact essential for building a coherent understanding of brain function.

In this repository, I aim to introduce highly useful concept which helps us to decipher complex nature of brain function in very different scenarios including resting state, motor activity, visual processing, BCI and so on. This useful concept is called as "Entropy" which is originated in thermodynamics and was later formalized within information theory as a fundamental measure of uncertainty, variability, and information content in a system. 

## Born of Entropy 
Entropy originated in thermodynamics as a measure of disorder or randomness in a physical system, introduced by Rudolf Clausius in the 1850s and later interpreted statistically by Ludwig Boltzmann. Boltzmann's formula,

$$
S = k \ln W
$$

where $S$ is entropy, $k$ is Boltzmann's constant, and $W$ is the number of microstates, quantifies the multiplicity of ways a system can arrange itself while maintaining the same macroscopic properties.

This thermodynamic concept laid the groundwork for its adaptation into information theory by Claude Shannon in 1948, who redefined entropy as a measure of uncertainty or information content in a message source, given by

$$
H = - \sum_i p_i \log_2 p_i
$$

for discrete probabilities $p_i$.
